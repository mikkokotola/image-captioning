@incollection{Attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@InProceedings{CIDEr,
author = {Vedantam, Ramakrishna and Lawrence Zitnick, C. and Parikh, Devi},
title = {CIDEr: Consensus-Based Image Description Evaluation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@InProceedings{SPICE,
author="Anderson, Peter
and Fernando, Basura
and Johnson, Mark
and Gould, Stephen",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="SPICE: Semantic Propositional Image Caption Evaluation",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="382--398",
abstract="There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?",
isbn="978-3-319-46454-1"
}



@article{ZagoruykoK16,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1605.07146},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07146},
  archivePrefix = {arXiv},
  eprint    = {1605.07146},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Coco,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll{\'a}r},
      journal   = {CoRR},
      volume    = {abs/1405.0312},
      year={2015},
      url       = {http://arxiv.org/abs/1605.07146},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{pascal-voc-2012,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"}	

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
        
@inproceedings{zhou2017scene,
    title={Scene Parsing through ADE20K Dataset},
    author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    year={2017}
    }
    
    
@article{zhou2016semantic,
  title={Semantic understanding of scenes through the ade20k dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  journal={arXiv preprint arXiv:1608.05442},
  year={2016}
}

@article{CocoStuff,
  author    = {Holger Caesar and
               Jasper R. R. Uijlings and
               Vittorio Ferrari},
  title     = {COCO-Stuff: Thing and Stuff Classes in Context},
  journal   = {CoRR},
  volume    = {abs/1612.03716},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.03716},
  archivePrefix = {arXiv},
  eprint    = {1612.03716},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CaesarUF16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VisualGenome,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  number={1},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@article{SemanticKITTI,
  author    = {Jens Behley and
               Martin Garbade and
               Andres Milioto and
               Jan Quenzel and
               Sven Behnke and
               Cyrill Stachniss and
               Juergen Gall},
  title     = {A Dataset for Semantic Segmentation of Point Cloud Sequences},
  journal   = {CoRR},
  volume    = {abs/1904.01416},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.01416},
  archivePrefix = {arXiv},
  eprint    = {1904.01416},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-01416.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{M2,
author = {Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
title = {Meshed-Memory Transformer for Image Captioning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{M2supplement,
author = {Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
title = {Meshed-Memory Transformer for Image Captioning, Supplementary Material},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
} 

@article{SurveyDL,
author = {Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
title = {A Comprehensive Survey of Deep Learning for Image Captioning},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3295748},
doi = {10.1145/3295748},
abstract = {Generating a description of an image is called image captioning. Image captioning requires recognizing the important objects, their attributes, and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep-learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey article, we aim to present a comprehensive review of existing deep-learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths, and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep-learning-based automatic image captioning.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {118},
numpages = {36},
keywords = {natural language processing, CNN, LSTM, deep learning, Image captioning, computer vision}
}

@article{XLinearAN,
  title={X-Linear Attention Networks for Image Captioning},
  author={Yingwei Pan and Ting Yao and Yehao Li and Tao Mei},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={10968-10977}
}

@InProceedings{ShowEdit,
author = {Sammani, Fawaz and Melas-Kyriazi, Luke},
title = {Show, Edit and Tell: A Framework for Editing Image Captions},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
} 

@InProceedings{ShowEdit_supp,
author = {Sammani, Fawaz and Melas-Kyriazi, Luke},
title = {Supplementary Material for Show, Edit and Tell: A Framework for Editing Image Captions},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
} 


@InProceedings{BottomUp,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@InProceedings{BottomUpSupplement,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering, supplementary materials},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 


@ARTICLE{FasterRCNN,  author={S. {Ren} and K. {He} and R. {Girshick} and J. {Sun}},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},   year={2017},  volume={39},  number={6},  pages={1137-1149},  doi={10.1109/TPAMI.2016.2577031}}

@InProceedings{FastRCNN,
author = {Girshick, Ross},
title = {Fast R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
} 

@InProceedings{RCNN,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
} 

@InProceedings{EfficientNet, title = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks}, author = {Tan, Mingxing and Le, Quoc}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6105--6114}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf}, url = { http://proceedings.mlr.press/v97/tan19a.html }, abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.} }

@InProceedings{SearchActivationF,
      title={Searching for Activation Functions}, 
      author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1710.05941},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      URL	= {https://arxiv.org/pdf/1710.05941.pdf}
}

@InProceedings{SelfAttentionForImageRec,
author = {Zhao, Hengshuang and Jia, Jiaya and Koltun, Vladlen},
title = {Exploring Self-Attention for Image Recognition},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{Prophet,
 author = {Liu, Fenglin and Ren, Xuancheng and Wu, Xian and Ge, Shen and Fan, Wei and Zou, Yuexian and Sun, Xu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1865--1876},
 publisher = {Curran Associates, Inc.},
 title = {Prophet Attention: Predicting Attention with Future Attention},
 url = {https://proceedings.neurips.cc/paper/2020/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf},
 volume = {33},
 year = {2020}
}

@InProceedings{AoA,
author = {Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
title = {Attention on Attention for Image Captioning},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{SelfCriticalTraining,
author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
title = {Self-Critical Sequence Training for Image Captioning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@InProceedings{DETR,
author="Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="End-to-End Object Detection with Transformers",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--229",
abstract="We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
isbn="978-3-030-58452-8"
}

@article{NFNets,
      title={High-Performance Large-Scale Image Recognition Without Normalization}, 
      author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},
      year={2021},
      eprint={2102.06171},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
	  journal   = {CoRR},
	  volume    = {abs/2102.06171},
	  url       = {http://arxiv.org/abs/2102.06171}
}

@InProceedings{MaskRCNN,
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
title = {Mask R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
} 

@inproceedings{LocalizedNarratives,
  author    = {Jordi Pont-Tuset and Jasper Uijlings and Soravit Changpinyo and Radu Soricut and Vittorio Ferrari},
  title     = {Connecting Vision and Language with Localized Narratives},
  booktitle = {ECCV},
  year      = {2020}
}

@article{PolyYOLO,
      title={Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3}, 
      author={Petr Hurtik and Vojtech Molek and Jan Hula and Marek Vajgl and Pavel Vlasanek and Tomas Nejezchleba},
	  journal   = {CoRR},
  	  volume    = {abs/2005.13243},      
      year={2020},
      url       = {http://arxiv.org/abs/2005.13243},
      eprint={2005.13243},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{YOLO9000,
author = {Redmon, Joseph and Farhadi, Ali},
title = {YOLO9000: Better, Faster, Stronger},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
} 

@article{YOLOv3,
      title={YOLOv3: An Incremental Improvement}, 
      author={Joseph Redmon and Ali Farhadi},
      journal   = {CoRR},
  	  volume    = {abs/1804.02767},      
      year={2018},
      url       = {http://arxiv.org/abs/1804.02767},
      eprint={1804.02767},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{ResNet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{MultiUpDown,
      title={3M: Multi-style image caption generation using Multi-modality features under Multi-UPDOWN model}, 
      author={Chengxi Li and Brent Harrison},
      journal   = {CoRR},
  	  volume    = {abs/2103.11186},      
      year={2021},
      url       = {http://arxiv.org/abs/2103.11186},
      eprint={2103.11186},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{StyleNet,
author = {Gan, Chuang and Gan, Zhe and He, Xiaodong and Gao, Jianfeng and Deng, Li},
title = {StyleNet: Generating Attractive Visual Captions With Styles},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
} 

@InProceedings{PersonalityCaptions,
author = {Shuster, Kurt and Humeau, Samuel and Hu, Hexiang and Bordes, Antoine and Weston, Jason},
title = {Engaging Image Captioning via Personality},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{LargeScalePretraining,
author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
title = {Exploring the Limits of Weakly Supervised Pretraining},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@InProceedings{Karpathy,
author = {Karpathy, Andrej and Fei-Fei, Li},
title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
} 

@article{ELU,
      title={Continuously Differentiable Exponential Linear Units}, 
      author={Jonathan T. Barron},
      journal={CoRR},
      volume={1704.07483},      
      year={2017},
      eprint={1704.07483},
      url={http://arxiv.org/abs/1704.07483},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{datatotext,
      title={Data-to-Text Generation with Iterative Text Editing}, 
      author={Zdenek Kasner and Ondrej Dusek},
	  journal={CoRR},
      volume={2011.01694},            
      year={2021},
      eprint={2011.01694},
      url={http://arxiv.org/abs/2011.01694},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{SENet,
author = {Hu, Jie and Shen, Li and Sun, Gang},
title = {Squeeze-and-Excitation Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@InProceedings{SENetsupp,
author = {Hu, Jie and Shen, Li and Sun, Gang},
title = {Squeeze-and-Excitation Networks, Supplementary Material},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@InProceedings{ResNeXt,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@article{PAWS,
      title={Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples}, 
      author={Mahmoud Assran and Mathilde Caron and Ishan Misra and Piotr Bojanowski and Armand Joulin and Nicolas Ballas and Michael Rabbat},
      year={2021},
      journal={CoRR},
      volume={2104.13963},
      url={http://arxiv.org/abs/2104.13963},
      eprint={2104.13963},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{DINO,
      title={Emerging Properties in Self-Supervised Vision Transformers}, 
      author={Mathilde Caron and Hugo Touvron and Ishan Misra and Herve Jegou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
      year={2021},
      journal={CoRR},
      volume={2104.14294},
      url={http://arxiv.org/abs/2104.14294},
      eprint={2104.14294},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{VisionTransformers,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2020},
      journal={CoRR},
      volume={2010.11929},
      url={http://arxiv.org/abs/2010.11929},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{JFT,
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@article{YOLOv4,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, 
      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
      year={2020},
      journal={CoRR},
      volume={2004.10934},
      url={http://arxiv.org/abs/2004.10934},
      eprint={2004.10934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{BridgingObjectAnchor,
      title={Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection}, 
      author={Shifeng Zhang and Cheng Chi and Yongqiang Yao and Zhen Lei and Stan Z. Li},
      year={2020},
      eprint={1912.02424},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      journal   = {arXiv},
      volume    = {abs/1912.02424}
}

@inproceedings{CSPNet,
  title={CSPNet: A new backbone that can enhance learning capability of CNN},
  author={Wang, Chien-Yao and Liao, Hong-Yuan Mark and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei and Yeh, I-Hau},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={390--391},
  year={2020}
}

@inproceedings{MnasNet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2820--2828},
  year={2019}
}

@InProceedings{MobileNetv2,
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{DeeperImageTransformers,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2103.17239},
  year={2021},
  eprint={1912.02424},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  volume    = {abs/2103.17239}
}

@inproceedings{EfficientDet,
  title={Efficientdet: Scalable and efficient object detection},
  author={Tan, Mingxing and Pang, Ruoming and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10781--10790},
  year={2020}
}

@inproceedings{FeaturePyramidNetworks,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2117--2125},
  year={2017}
}

@InProceedings{RetinaNet,
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
title = {Focal Loss for Dense Object Detection},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
} 

@inproceedings{FewShot,
  title={Few-Shot NLG with Pre-Trained Language Model},
  author={Chen, Zhiyu and Eavani, Harini and Chen, Wenhu and Liu, Yinyin and Wang, William Yang},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={183--190},
  year={2020}
}

@article{GPT3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  volume    = {abs/2005.14165}
}

@article{ImageNet21k_Pretraining,
      title={ImageNet-21K Pretraining for the Masses}, 
      author={Tal Ridnik and Emanuel Ben-Baruch and Asaf Noy and Lihi Zelnik-Manor},
      journal={arXiv preprint arXiv:2104.10972},
      year={2021},
      eprint={2104.10972},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      volume    = {abs/2104.10972}
}

@article{Imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{Flickr30k,
    author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
    title = "{From image descriptions to visual denotations: New similarity metrics
                    for semantic inference over event descriptions}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {2},
    pages = {67-78},
    year = {2014},
    month = {02},
    abstract = "{We propose to use the visual denotations of
                    linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show
                    to be at least as beneficial as distributional similarities for two tasks that
                    require semantic inference. To compute these denotational similarities, we
                    construct a denotation graph, i.e. a subsumption
                    hierarchy over constituents and their denotations, based on a large corpus of
                    30K images and 150K descriptive captions.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00166},
    url = {https://doi.org/10.1162/tacl\_a\_00166},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00166/1566848/tacl\_a\_00166.pdf},
}
@inproceedings{Flickr30kentities,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2641--2649},
  year={2015}
}

@inproceedings{InceptionV4,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@book{DeepLearningBook,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}